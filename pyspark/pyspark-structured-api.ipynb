{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark_structured_api.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRD4AWvXv3bd"
      },
      "source": [
        "### Welcome to the Colab Spark Tutorial.\n",
        "\n",
        "We will be using Spark a few times in this course, and the _colab_ environment provides the compute (for 12 hours at a time) we need, along with this wonderful web-based notebook.\n",
        "\n",
        "Today we will be configuring PySpark and exploring the SparkSQL features in relation to the Spark API\n",
        "\n",
        "Source material includes [[1](https://opensource.com/article/19/3/apache-spark-and-dataframes-tutorial)]\n",
        "\n",
        "Sections:\n",
        "\n",
        " 1. Configuring your _colab_\n",
        " 2. Using PySpark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dObl4yHR4aRm"
      },
      "source": [
        "Firstly, we need to configure the _colab_ instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy0a51RqrDlC"
      },
      "source": [
        "!lsb_release -a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX6iQmbtrIvl"
      },
      "source": [
        "!apt-get update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm6zycFEsLeI"
      },
      "source": [
        "# Install java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAd3j6fJq1Ex"
      },
      "source": [
        "# get spark \n",
        "VERSION='3.2.2'\n",
        "!wget https://dlcdn.apache.org/spark/spark-$VERSION/spark-$VERSION-bin-hadoop3.2.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Wadv8_tsEQw"
      },
      "source": [
        "# decompress spark\n",
        "!tar xf spark-$VERSION-bin-hadoop3.2.tgz\n",
        "\n",
        "# install python package to help with system paths\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l60O1BJzuf_h"
      },
      "source": [
        "# Let Colab know where the java and spark folders are\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/spark-{VERSION}-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbpi4WDgu_VL"
      },
      "source": [
        "# add pyspark to sys.path using findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# get a spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87le9nge8TLK"
      },
      "source": [
        "Let's download some url data (\"Anonymized 120-day subset of the ICML-09 URL data containing 2.4 million examples and 3.2 million features\" [UCI](https://archive.ics.uci.edu/ml/datasets/URL+Reputation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6SHTRfC6pFS"
      },
      "source": [
        "! wget http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\n",
        "! wget http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbToU3UA6q_o"
      },
      "source": [
        "!gunzip kddcup.data_10_percent.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAr0nnqG6u8v"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('kddcup.data_10_percent', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLdspkQ1844Q"
      },
      "source": [
        "df[2].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeHGThDKAZpc"
      },
      "source": [
        "raw_rdd = spark.sparkContext.textFile('kddcup.data_10_percent').cache()\n",
        "raw_rdd.take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMHP2KnWA0W-"
      },
      "source": [
        "csv_rdd = raw_rdd.map(lambda row: row.split(\",\"))\n",
        "print(csv_rdd.take(2))\n",
        "print(type(csv_rdd))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY3tARyYPdL4"
      },
      "source": [
        "Read the csv directly into a spark dataframe by defining a schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhLhdHB2BP48"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "parsed_rdd = csv_rdd.map(lambda r: Row(\n",
        "    duration=int(r[0]),\n",
        "    protocol_type=r[1],\n",
        "    service=r[2],\n",
        "    flag=r[3],\n",
        "    src_bytes=int(r[4]),\n",
        "    dst_bytes=int(r[5]),\n",
        "    wrong_fragment=int(r[7]),\n",
        "    urgent=int(r[8]),\n",
        "    hot=int(r[9]),\n",
        "    num_failed_logins=int(r[10]),\n",
        "    num_compromised=int(r[12]),\n",
        "    su_attempted=r[14],\n",
        "    num_root=int(r[15]),\n",
        "    num_file_creations=int(r[16]),\n",
        "    label=r[-1]\n",
        "    )\n",
        ")\n",
        "parsed_rdd.take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K8Dj77vPm9R"
      },
      "source": [
        "Convert the RDD to a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTZqz_CqBhRB"
      },
      "source": [
        "df = spark.createDataFrame(parsed_rdd)\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQU24SGs6YZS"
      },
      "source": [
        "from pyspark.sql import functions as f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNNA7WHlCD_u"
      },
      "source": [
        "# register a temporary table to query against.\n",
        "df.registerTempTable('data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p12ji90-PvFv"
      },
      "source": [
        "# Comparing SQL to API "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB3QG86AIc9Q"
      },
      "source": [
        "---\n",
        "#0. Select columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHM-LsliHwen"
      },
      "source": [
        "select = spark.sql(\"\"\"SELECT protocol_type, service\n",
        "                      FROM data\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_ZSeIJQHw8b"
      },
      "source": [
        "select.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqO6_d1MHwzH"
      },
      "source": [
        "select_spark = df.select('protocol_type', 'service')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUftQKIHHwTl"
      },
      "source": [
        "select_spark.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgFosi_oJFJb"
      },
      "source": [
        "#### OR using a list also works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKNEqFSGJIkx"
      },
      "source": [
        "select_spark = df.select(['protocol_type', 'service'])\n",
        "select_spark.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UexKUefVJSOQ"
      },
      "source": [
        "---\n",
        "# 1. select as alias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7j_LBFWJchg"
      },
      "source": [
        "alias = spark.sql(\"\"\"SELECT protocol_type,\n",
        "                            label as flag\n",
        "                     FROM data\n",
        "                  \"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA5FWkOyJczb"
      },
      "source": [
        "alias.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibkdCi1xJcmV"
      },
      "source": [
        "alias_spark = df.select('protocol_type', 'label').withColumnRenamed('label', 'flag')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysR___SFJceU"
      },
      "source": [
        "alias_spark.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VkTVQ5VKZ5s"
      },
      "source": [
        "#### OR using dataframe column-objects with .alias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd7lAhSmKRAX"
      },
      "source": [
        "alias_spark = df.select(df.protocol_type, df.label.alias('flag'))\n",
        "alias_spark.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8TdxqQz_DXR"
      },
      "source": [
        "# 2. group by, count, order by"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4le_EnCCWY0"
      },
      "source": [
        "protocols = spark.sql(\"\"\"\n",
        "      SELECT protocol_type, count(*) as freq\n",
        "      FROM data\n",
        "      GROUP BY protocol_type\n",
        "      ORDER BY 2 DESC\n",
        "                           \"\"\")\n",
        "protocols.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmp9bXaRCLjO"
      },
      "source": [
        "df.groupBy('protocol_type').count().orderBy('count', ascending=False).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8T7v7-hCueD"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06jXlwmU_O0c"
      },
      "source": [
        "---\n",
        "# 3. group by, count, order by (using agg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYHRoj36EV6B"
      },
      "source": [
        "labels = spark.sql(\"\"\"\n",
        "  SELECT label, count(*) as freq\n",
        "  FROM data\n",
        "  GROUP BY label\n",
        "  ORDER BY 2 DESC\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmZtqo1V-Ccq"
      },
      "source": [
        "labels.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isahL3zB-F9p"
      },
      "source": [
        "labels_spark = df.groupBy('label')\\\n",
        "                .agg(f.count(f.lit(1))\\\n",
        "                    .alias('freq'))\\\n",
        "                    .orderBy('freq', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWuxd0wu-Fz9"
      },
      "source": [
        "labels_spark.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gCGSzGr-Azf"
      },
      "source": [
        "---\n",
        "#4. case, group by, count, order by"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyozq1CnEbBA"
      },
      "source": [
        "attack_protocol = spark.sql(\"\"\"\n",
        "                           SELECT\n",
        "                             protocol_type,\n",
        "                             CASE label\n",
        "                               WHEN 'normal.' THEN 'no attack'\n",
        "                               ELSE 'attack'\n",
        "                             END AS state,\n",
        "                             COUNT(*) as freq\n",
        "                           FROM data\n",
        "                           GROUP BY protocol_type, state\n",
        "                           ORDER BY 3 DESC\n",
        "                           \"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjg6bbh7EwrH"
      },
      "source": [
        "attack_protocol.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTAEnJp-XDlw"
      },
      "source": [
        "att_prot_spark = df.withColumn('state', f.when(df.label=='normal.', 'no attack').otherwise('attack'))\\\n",
        "                  .groupBy('protocol_type', 'state')\\\n",
        "                  .agg(f.count(f.lit(1)).alias('freq'))\\\n",
        "                  .orderBy('freq', ascending=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AinIOikp6DB7"
      },
      "source": [
        "att_prot_spark.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvLHycsW_u4J"
      },
      "source": [
        "---\n",
        "#5. group by, aggregations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1GPRBt47ODq"
      },
      "source": [
        "attack_stats = spark.sql(\"\"\"\n",
        "                          SELECT\n",
        "                            protocol_type,\n",
        "                            CASE label\n",
        "                              WHEN 'normal.' THEN 'no attack'\n",
        "                              ELSE 'attack'\n",
        "                            END AS state,\n",
        "                            COUNT(*) as total_freq,\n",
        "                            ROUND(AVG(src_bytes), 2) as mean_src_bytes,\n",
        "                            ROUND(AVG(dst_bytes), 2) as mean_dst_bytes,\n",
        "                            ROUND(AVG(duration), 2) as mean_duration,\n",
        "                            SUM(num_failed_logins) as total_failed_logins,\n",
        "                            SUM(num_compromised) as total_compromised,\n",
        "                            SUM(num_file_creations) as total_file_creations,\n",
        "                            SUM(su_attempted) as total_root_attempts,\n",
        "                            SUM(num_root) as total_root_acceses\n",
        "                          FROM data\n",
        "                          GROUP BY protocol_type, state\n",
        "                          ORDER BY 3 DESC\n",
        "                          \"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpzYvR-Y9vaw"
      },
      "source": [
        "attack_stats.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig3lTdSb90Ey"
      },
      "source": [
        "attack_stats_spark = df.withColumn('state', f.when(df.label=='normal.', 'no attack').otherwise('attack'))\\\n",
        ".groupBy('protocol_type', 'state')\\\n",
        ".agg(f.count(f.lit(1)).alias('total_freq'),\n",
        "     f.avg('src_bytes').alias('mean_src_bytes'),\n",
        "     f.avg('dst_bytes').alias('mean_dst_bytes'),\n",
        "     f.avg('duration').alias('mean_duration'),\n",
        "     f.sum('num_failed_logins').alias('total_failed_logins'),\n",
        "     f.sum('num_compromised').alias('total_compromised'),\n",
        "     f.sum('num_file_creations').alias('total_file_creations'),\n",
        "     f.sum('su_attempted').alias('total_root_attempts'),\n",
        "     f.sum('num_root').alias('total_root_acceses'),\n",
        "     )\\\n",
        "     .orderBy('total_freq', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGJt0IrxAIXh"
      },
      "source": [
        "attack_stats_spark.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baMxExBFEwNy"
      },
      "source": [
        "---\n",
        "# 6. filter, group by "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkagtnddEvgs"
      },
      "source": [
        "tcp_attack_stats = spark.sql(\"\"\"\n",
        "                              SELECT\n",
        "                                service,\n",
        "                                label as attack_type,\n",
        "                                COUNT(*) as total_freq,\n",
        "                                ROUND(AVG(duration), 2) as mean_duration,\n",
        "                                SUM(num_failed_logins) as total_failed_logins,\n",
        "                                SUM(num_file_creations) as total_file_creations,\n",
        "                                SUM(su_attempted) as total_root_attempts,\n",
        "                                SUM(num_root) as total_root_acceses\n",
        "                              FROM data\n",
        "                              WHERE protocol_type = 'tcp'\n",
        "                              AND label != 'normal.'\n",
        "                              GROUP BY service, attack_type\n",
        "                              ORDER BY total_freq DESC\n",
        "                              \"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LPlGI18AlO2"
      },
      "source": [
        "tcp_attack_stats.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV9GkI6YE4Y9"
      },
      "source": [
        "tcp_attack_stats_spark = df.filter((df.protocol_type  == \"tcp\") & (df.label  != \"normal.\")).groupBy('service', df.label.alias('attack_type'))\\\n",
        ".agg(f.count(f.lit(1)).alias('total_freq'),\n",
        "     f.avg('duration').alias('mean_duration'),\n",
        "     f.sum('num_failed_logins').alias('total_failed_logins'),\n",
        "     f.sum('num_file_creations').alias('total_file_creations'),\n",
        "     f.sum('su_attempted').alias('total_root_attempts'),\n",
        "     f.sum('num_root').alias('total_root_acceses'))\\\n",
        ".orderBy('total_freq', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksvTWt7FFspS"
      },
      "source": [
        "tcp_attack_stats_spark.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTCIdHu5NGK2"
      },
      "source": [
        "---\n",
        "#7. sub-queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lvC5LyrLOyK"
      },
      "source": [
        "tcp_attack_stats = spark.sql(\"\"\"\n",
        "                              SELECT\n",
        "                                t.service,\n",
        "                                t.attack_type,\n",
        "                                t.total_freq\n",
        "                              FROM\n",
        "                              (SELECT\n",
        "                                service,\n",
        "                                label as attack_type,\n",
        "                                COUNT(*) as total_freq,\n",
        "                                ROUND(AVG(duration), 2) as mean_duration,\n",
        "                                SUM(num_failed_logins) as total_failed_logins,\n",
        "                                SUM(num_file_creations) as total_file_creations,\n",
        "                                SUM(su_attempted) as total_root_attempts,\n",
        "                                SUM(num_root) as total_root_acceses\n",
        "                              FROM data\n",
        "                              WHERE protocol_type = 'tcp'\n",
        "                              AND label != 'normal.'\n",
        "                              GROUP BY service, attack_type\n",
        "                              ORDER BY total_freq DESC) as t\n",
        "                                WHERE t.mean_duration > 0\n",
        "                              \"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df0x3JcVNqHY"
      },
      "source": [
        "tcp_attack_stats.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MBu9zi4NuRH"
      },
      "source": [
        "tcp_attack_stats_spark = df.filter((df.protocol_type  == \"tcp\") & (df.label  != \"normal.\"))\\\n",
        ".groupBy('service', df.label.alias('attack_type'))\\\n",
        ".agg(f.count(f.lit(1)).alias('total_freq'),\n",
        "     f.avg('duration').alias('mean_duration'),\n",
        "     f.sum('num_failed_logins').alias('total_failed_logins'),\n",
        "     f.sum('num_file_creations').alias('total_file_creations'),\n",
        "     f.sum('su_attempted').alias('total_root_attempts'),\n",
        "     f.sum('num_root').alias('total_root_acceses'))\\\n",
        ".orderBy('total_freq', ascending=False)\\\n",
        ".filter(f.col('mean_duration') > 0)\\\n",
        ".select('service', 'attack_type', 'total_freq')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q-gcTdgOWJX"
      },
      "source": [
        "tcp_attack_stats_spark.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YS3-EGdX4NOf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}