{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce\n",
    "\n",
    "This notebook is performing map reduce in a simplified manner in Python. Distribution of compute to different nodes is not done here; the purpose rather is to explore how to implement a map or reduce function, assuming that the functionality is provided akin to the libraries mentioned in [Dean and Ghemawat](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf).\n",
    "\n",
    "\n",
    "This notebook comprises a section defining identity mappers and reducers, along with a `run` method which you may change if necessary. An intermediate sort function is also provided. \n",
    "\n",
    "Implement the `mapper` and `reducer` in the Term Vectors section, and use the run cell as provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "%config Completer.use_jedi = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty MAPPER\n",
    "def mapper(key, value):\n",
    "    \"\"\"\n",
    "    Our user defined mapper function .\n",
    "    : param key : \n",
    "    : param value : \n",
    "    \"\"\"\n",
    "    yield (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty REDUCER\n",
    "\n",
    "def reducer(key , list_value):\n",
    "    \"\"\"\n",
    "    User defined reducer.\n",
    "    : param key : \n",
    "    : param list_value :\n",
    "    \"\"\"\n",
    "    yield (key, list_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cleaner(line):\n",
    "    # lowercase all words and get alphabetical char only and keeping\n",
    "    # apostrophe for time being\n",
    "    words = re.findall(r'[a-z\\']+' , line.lower())\n",
    "    for word in words :\n",
    "        # we will omit apostrophe ' s assuming users won't type them in a search\n",
    "        word = word.replace(\"'\" , '')\n",
    "        if not (word is '' or word in stopwords.words('english')):\n",
    "            yield word\n",
    "\n",
    "\n",
    "    \n",
    "def intermediate_sort(data):\n",
    "    \"\"\"\n",
    "    collect by key \n",
    "    \"\"\"\n",
    "    data = sorted ( data )\n",
    "    return [(k, list(tuple(zip(*g))[1])) for k, g  in groupby(data , itemgetter(0))]\n",
    "\n",
    "    \n",
    "\n",
    "def run(sources_dict):\n",
    "    \"\"\"\n",
    "    Since we are focusing on the mapper and reducer functions here we have to\n",
    "    provide the boiler plate code that a MapReduce library typically would . This\n",
    "    function does that in a simple way (we ignore distributing it for now).\n",
    "    : param sources_dict : dictionary where (key,fqfilename), for example ('doc_id','/home/fileX')\n",
    "    \"\"\"\n",
    "    map_result =[]\n",
    "    reduce_result =[]\n",
    "    # open the files and apply map to each of them ( could be done in parallel ,\n",
    "    # but we prefer to keep it simple ) .\n",
    "    for k , v in sources_dict.items():\n",
    "        # do map per source\n",
    "        # this could happen in its own process / worker typically\n",
    "        f = open(v, 'r')\n",
    "        map_result += list(mapper(k, f.read()))\n",
    "        f.close()\n",
    "#         ::alt\n",
    "#          with open(v, 'r') as f:\n",
    "#             for line in f.readlines():\n",
    "#                 map_result += list(mapper(k, line))\n",
    "    # this would be written to disk in the original paradigm ,\n",
    "    # but we keep it in memory for ease of use\n",
    "    intermediate_result = intermediate_sort(map_result)\n",
    "    # now that the data has been ' collected ' and grouped by key it can be handed\n",
    "    # to the reducers . They would run over partitions or chunks usually , but we\n",
    "    # will just iterate through the keys we have and call them\n",
    "    for elem in intermediate_result:\n",
    "        reduce_result.append(list(reducer(elem [0], elem [1])))\n",
    "    return map_result, intermediate_result, reduce_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('D1', ['D1 : the cat sat on the mat\\n'])],\n",
       " [('D2', ['D2 : the dog sat on the log\\n'])]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXAMPLE\n",
    "!mkdir -p input/\n",
    "!echo -e 'D1 : the cat sat on the mat' > input/d1.txt\n",
    "!echo -e 'D2 : the dog sat on the log' > input/d2.txt\n",
    "\n",
    "_, _, res = run({'D1': 'input/d1.txt' , 'D2': 'input/d2.txt'})\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Vector\n",
    "\n",
    "The paper states:\n",
    "\n",
    "> Term-Vector per Host: A term vector summarizes the\n",
    "most important words that occur in a document or a set\n",
    "of documents as a list of 〈word, frequency〉 pairs. The\n",
    "map function emits a 〈hostname, term vector〉\n",
    "pair for each input document (where the hostname is\n",
    "extracted from the URL of the document). The re-\n",
    "duce function is passed all per-document term vectors\n",
    "for a given host. It adds these term vectors together,\n",
    "throwing away infrequent terms, and then emits a final\n",
    "〈hostname, term vector〉 pair.\n",
    "\n",
    "As for \n",
    "\n",
    "> throwing away infrequent terms\n",
    "\n",
    "Write your code in such a way that only terms occurring at least twice are retained.\n",
    "\n",
    "Hint: \n",
    "  * Consider how they use the word 'frequency' elsewhere in the paper.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your mapper\n",
    "def mapper(key, value):\n",
    "    yield (key, value)\n",
    "\n",
    "def reducer(key , list_value):\n",
    "    yield (key, list_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, res = run({'www.somesite.com/page/1': 'page1.txt', 'www.somesite.com/page/2': 'page2.txt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdt-a3",
   "language": "python",
   "name": "bdt-a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
